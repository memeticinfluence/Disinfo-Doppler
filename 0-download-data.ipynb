{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Pushshift Reddit Data\n",
    "See this on [Github](https://github.com/yinleon/doppler_tutorials/blob/master/1-download-data.ipynb), [NbViewer](https://nbviewer.jupyter.org/github/yinleon/doppler_tutorials/blob/master/0-download-data.ipynb)<br>\n",
    "By Leon Yin 2019-04-05<br>\n",
    "This Notebook collects subreddit metadata from PushShift's REST API, and downloads images from Reddit using requests.\n",
    "\n",
    "## API Endpoints\n",
    "A few helpful notes about PushShift's API.:\n",
    "\n",
    "To get a subset of files here:<br>\n",
    "`http://api.pushshift.io/reddit/submission/search/?subreddit=politics&size=500&sort=desc`\n",
    "\n",
    "Latest data is accessible here here:<br>\n",
    "`http://api.pushshift.io/reddit/submission/search/?subreddit=politics&after=2h&size=500`\n",
    "\n",
    "Get everything under the sun here:<br>\n",
    "`files.pushshift.io/reddit/submissions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import requests\n",
    "import itertools\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "import pandas as pd\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[ 502, 503, 504 ])\n",
    "s.mount('http://', HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trick in data engineering is creating a dictionary with variables such as the destination of the output file. We will use functions like `get_context` throughout this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(subreddit):\n",
    "    '''\n",
    "    Where will data be saved?\n",
    "    '''\n",
    "    sub_dir = os.path.join(data_dir, subreddit)\n",
    "    media_dir =  os.path.join(data_dir, 'media')\n",
    "    file_subreddit = os.path.join(sub_dir, 'posts.json.gz')\n",
    "    file_subreddit_media = os.path.join(sub_dir, 'media.json.gz')\n",
    "    \n",
    "    for _dir in [data_dir, sub_dir, media_dir]:\n",
    "        os.makedirs(_dir, exist_ok=True)\n",
    "        \n",
    "    context = {\n",
    "        'data_dir' : data_dir,\n",
    "        'sub_dir' : sub_dir,\n",
    "        'media_dir' : media_dir,\n",
    "        'file_subreddit' : file_subreddit,\n",
    "        'file_subreddit_media' : file_subreddit_media\n",
    "    }\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query PushShift\n",
    "These next two functions can be re-used for other purposes where you may need data from PushShift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_api_endpoint(subreddit, size, ascending, last_record_date, verbose):\n",
    "    '''\n",
    "    An easy API endpoint builder for PushShift's Reddit API.\n",
    "    '''\n",
    "    url_base = ('http://api.pushshift.io/reddit/submission/search/'\n",
    "               f'?subreddit={ subreddit }&size={ size }')\n",
    "    if not last_record_date:\n",
    "        url = url_base\n",
    "    else:\n",
    "        if ascending:\n",
    "            url = url_base + f'&after={ last_record_date + 1 }&sort=asc'\n",
    "        else:\n",
    "            url = url_base + f'&before={ last_record_date - 1 }&sort=desc'\n",
    "    if verbose:\n",
    "        print(url)\n",
    "    return url\n",
    "\n",
    "\n",
    "def download_subreddit_posts(subreddit, size=5000, ascending=False, \n",
    "                             start_date=False, seen_ids=set(), \n",
    "                             display_every_x_iterations=20,\n",
    "                             verbose=True):\n",
    "    '''\n",
    "    Downloads subreddit data.\n",
    "    To go back in time from `start_date`, set ascending to False.\n",
    "    To go forward from time `state_date`, set ascneding to True.\n",
    "    Skipps seen_ids\n",
    "    '''\n",
    "    if not isinstance(seen_ids, set):\n",
    "        raise \"seen_ids needs to be a set!\"\n",
    "    i = 0\n",
    "    records = []\n",
    "    last_record_date = start_date\n",
    "    try:\n",
    "        while True:\n",
    "            # Buld the url\n",
    "            url = build_api_endpoint(subreddit, size, ascending, last_record_date, \n",
    "                                     verbose if i % display_every_x_iterations == 0 else False)\n",
    "\n",
    "            # make the HTTP request to the API\n",
    "            r = s.get(url)\n",
    "            resp = r.json()\n",
    "            data = resp.get('data')\n",
    "\n",
    "            # check which records were returned by the API and which are new?\n",
    "            # if there are no new IDs, then we're done!\n",
    "            paginated_ids = {row.get(\"id\") for row in data}\n",
    "            new_ids = paginated_ids - seen_ids\n",
    "            if len(new_ids) == 0:\n",
    "                break\n",
    "\n",
    "            # add new records to existing records. \n",
    "            new_records = [row for row in data if row['id'] in new_ids]\n",
    "            records.extend(new_records)\n",
    "\n",
    "            # collect all records created before the last record's date.\n",
    "            last_record = data[-1]\n",
    "            last_record_date = last_record.get('created_utc')\n",
    "            i += 1\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        if verbose:\n",
    "            print(\"Cancelled early\")\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call these functions to collect data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Exists\n",
      "294611 Records exist\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=pewdiepiesubmissions&size=5000&after=1555134672&sort=asc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=pewdiepiesubmissions&size=5000&before=1551823502&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=pewdiepiesubmissions&size=5000&before=1551618603&sort=desc\n",
      "Cancelled early\n",
      "collected 40263 records\n",
      "\n",
      "****************\n",
      "N = 334874\n",
      "Start Date = 2019-02-28 22:47:45\n",
      "End Date = 2019-04-13 15:05:37\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "subreddit = config.subreddit # change this in config.py\n",
    "context = get_context(subreddit)\n",
    "\n",
    "if os.path.exists(context['file_subreddit']):\n",
    "    print('File Exists')\n",
    "    df = pd.read_json(context['file_subreddit'], \n",
    "                      lines=True, orient='records',\n",
    "                      compression='gzip')\n",
    "    \n",
    "    print(f'{ len(df) } Records exist')\n",
    "    min_date = df.created_utc.min()\n",
    "    max_date = df.created_utc.max()\n",
    "    seen_ids = set(df.id.unique())\n",
    "    \n",
    "    newer_records = download_subreddit_posts(subreddit, verbose=verbose, \n",
    "                                             seen_ids=seen_ids,\n",
    "                                             ascending=True, \n",
    "                                             start_date=max_date)\n",
    "    older_records = download_subreddit_posts(subreddit, verbose=verbose, \n",
    "                                             seen_ids=seen_ids,\n",
    "                                             ascending=False, \n",
    "                                             start_date=min_date)\n",
    "    newer_records.extend(older_records)\n",
    "    _df = pd.DataFrame(newer_records)\n",
    "    if verbose:\n",
    "        print(f\"collected { len(_df) } records\")\n",
    "    df = df.append(_df, sort=False)\n",
    "    df.drop_duplicates(subset=['id'], inplace=True)\n",
    "    df.sort_values(by=['created_utc'], ascending=False, inplace=True)\n",
    "    df.to_json(context['file_subreddit'], lines=True, orient='records', compression='gzip')\n",
    "\n",
    "else:\n",
    "    print(\"New Subreddit\")\n",
    "    records = download_subreddit_posts(subreddit, verbose=verbose, size=5000)\n",
    "    if verbose:\n",
    "        print(f\"collected { len(records) } records\")\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_json(context['file_subreddit'], lines=True, orient='records', compression='gzip')\n",
    "\n",
    "if verbose:\n",
    "    # Summary stats\n",
    "    print('\\n****************')\n",
    "    df['created_at'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "    print(f\"N = { len(df) }\\n\"\n",
    "          f\"Start Date = { df['created_at'].min() }\\n\"\n",
    "          f\"End Date = { df['created_at'].max() }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_media(url, f):\n",
    "    '''\n",
    "    Downloads an image from the net and calcualtes the dhash\n",
    "    '''\n",
    "    if os.path.exists(f):\n",
    "        # is th image exists, don't download it again.\n",
    "        # calculate the size and hash\n",
    "        img_size = os.path.getsize(f)\n",
    "        if img_size != 0:\n",
    "            # read the image and calculate the hash\n",
    "            img = Image.open(f)\n",
    "            dhash = str(imagehash.dhash(img, hash_size=8))\n",
    "            \n",
    "            return dhash, img_size\n",
    "\n",
    "    # Download the image\n",
    "    r = s.get(url, stream=True)\n",
    "    if not r.status_code == 200:\n",
    "        return 'NOHASH', 0\n",
    "    \n",
    "    # download the image locally\n",
    "    with open(f, 'wb') as file:\n",
    "        r.raw.decode_content = True\n",
    "        shutil.copyfileobj(r.raw, file)\n",
    "    \n",
    "    # calculate the hash\n",
    "    img = Image.open(f)\n",
    "    img_size = os.path.getsize(f)\n",
    "    dhash = str(imagehash.dhash(img, hash_size=8))\n",
    "    \n",
    "    return dhash, img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_media_context(image, context):\n",
    "    '''\n",
    "    Establishes where media files will be saved.\n",
    "    '''\n",
    "    image_id = image['id']\n",
    "    pos_images = image.get('resolutions')\n",
    "    if pos_images:\n",
    "        largest_image = pos_images[-1]\n",
    "    else:\n",
    "        # no images...\n",
    "        return None, None\n",
    "    \n",
    "    # where is the image to be downlaoded?\n",
    "    img_url = largest_image.get('url')\n",
    "    img_url = img_url.replace('&amp;', '&')\n",
    "    \n",
    "    # what is the file  extension?\n",
    "    _, ext = os.path.splitext(img_url.split('?')[0])\n",
    "    ext = ext.replace('jpeg', 'jpg')\n",
    "    \n",
    "    # where will the images be downloaded locally?\n",
    "    dir_img = os.path.join(context['media_dir'], \n",
    "                           image_id[:2].lower(), \n",
    "                           image_id[2:4].lower())\n",
    "    \n",
    "    f_img = os.path.join(dir_img, image_id + ext)\n",
    "    os.makedirs(dir_img, exist_ok=True)\n",
    "    \n",
    "    return img_url, f_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1898it [01:02, 43.43it/s]/home/chino/miniconda3/lib/python3.7/site-packages/PIL/Image.py:968: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  ' expressed in bytes should be converted ' +\n",
      "71337it [10:58, 122.27it/s]"
     ]
    }
   ],
   "source": [
    "img_meta = []\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    preview = row.get('preview')\n",
    "    if isinstance(preview, dict):\n",
    "        images = preview.get('images')\n",
    "        if not images:\n",
    "            continue\n",
    "        for img in images:\n",
    "            r = row.copy()\n",
    "            img_url, f_img = get_media_context(img, context)\n",
    "            if not img_url:\n",
    "                continue\n",
    "            d_hash, img_size = download_media(img_url, f_img)\n",
    "            if img_size != 0:\n",
    "                r['deleted'] = False\n",
    "                r['d_hash'] = d_hash\n",
    "                r['f_img'] = f_img \n",
    "                r['img_size'] = img_size\n",
    "            else:\n",
    "                r['deleted'] = True\n",
    "                r['d_hash'] = d_hash\n",
    "                r['f_img'] = f_img \n",
    "                r['img_size'] = img_size\n",
    "            img_meta.append(r.to_dict())\n",
    "            \n",
    "                \n",
    "df_img_meta = pd.DataFrame(img_meta)\n",
    "df_img_meta.to_json(context['file_subreddit_media'], \n",
    "                    lines=True, orient='records',\n",
    "                    compression='gzip')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_img_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(f_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
