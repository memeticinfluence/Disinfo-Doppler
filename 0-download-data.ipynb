{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushshift Reddit Data\n",
    "This Notebook collects subreddit data from PushShift and downloads images from Reddit.\n",
    "\n",
    "## API Endpoints\n",
    "Get a subset of files here:<br>\n",
    "`http://api.pushshift.io/reddit/submission/search/?subreddit=politics&size=500&sort=desc`\n",
    "\n",
    "Latest here:<br>\n",
    "`http://api.pushshift.io/reddit/submission/search/?subreddit=politics&after=2h&size=500`\n",
    "\n",
    "Get everything under the sun here:<br>\n",
    "`files.pushshift.io/reddit/submissions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import requests\n",
    "import itertools\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "import pandas as pd\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[ 502, 503, 504 ])\n",
    "s.mount('http://', HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(subreddit):\n",
    "    '''\n",
    "    Where will data be saved?\n",
    "    '''\n",
    "    sub_dir = os.path.join(data_dir, subreddit)\n",
    "    media_dir =  os.path.join(data_dir, 'media')\n",
    "    file_subreddit = os.path.join(sub_dir, 'posts.csv.gz')\n",
    "    file_subreddit_media = os.path.join(sub_dir, 'media.csv.gz')\n",
    "    \n",
    "    for _dir in [data_dir, sub_dir, media_dir]:\n",
    "        os.makedirs(_dir, exist_ok=True)\n",
    "        \n",
    "    context = {\n",
    "        'data_dir' : data_dir,\n",
    "        'sub_dir' : sub_dir,\n",
    "        'media_dir' : media_dir,\n",
    "        'file_subreddit' : file_subreddit,\n",
    "        'file_subreddit_media' : file_subreddit_media\n",
    "    }\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query PushShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_api_endpoint(subreddit, size, ascending, last_record_date, verbose):\n",
    "    '''\n",
    "    An easy API endpoint builder for PushShift's Reddit API.\n",
    "    '''\n",
    "    url_base = ('http://api.pushshift.io/reddit/submission/search/'\n",
    "               f'?subreddit={ subreddit }&size={ size }')\n",
    "    if not last_record_date:\n",
    "        url = url_base\n",
    "    else:\n",
    "        if ascending:\n",
    "            url = url_base + f'&after={ last_record_date + 1 }&sort=asc'\n",
    "        else:\n",
    "            url = url_base + f'&before={ last_record_date - 1 }&sort=desc'\n",
    "    if verbose:\n",
    "        print(url)\n",
    "    return url\n",
    "\n",
    "\n",
    "def download_subreddit_posts(subreddit, size=5000, ascending=False, \n",
    "                             start_date=False, seen_ids=set(), \n",
    "                             display_every_x_iterations=20,\n",
    "                             verbose=True):\n",
    "    '''\n",
    "    Downloads subreddit data.\n",
    "    To go back in time from `start_date`, set ascending to False.\n",
    "    To go forward from time `state_date`, set ascneding to True.\n",
    "    Skipps seen_ids\n",
    "    '''\n",
    "    if not isinstance(seen_ids, set):\n",
    "        raise \"seen_ids needs to be a set!\"\n",
    "    i = 0\n",
    "    records = []\n",
    "    last_record_date = start_date\n",
    "    try:\n",
    "        while True:\n",
    "            # Buld the url\n",
    "            url = build_api_endpoint(subreddit, size, ascending, last_record_date, \n",
    "                                     verbose if i % display_every_x_iterations == 0 else False)\n",
    "\n",
    "            # make the HTTP request to the API\n",
    "            r = s.get(url)\n",
    "            resp = r.json()\n",
    "            data = resp.get('data')\n",
    "\n",
    "            # check which records were returned by the API and which are new?\n",
    "            # if there are no new IDs, then we're done!\n",
    "            paginated_ids = {row.get(\"id\") for row in data}\n",
    "            new_ids = paginated_ids - seen_ids\n",
    "            if len(new_ids) == 0:\n",
    "                break\n",
    "\n",
    "            # add new records to existing records. \n",
    "            new_records = [row for row in data if row['id'] in new_ids]\n",
    "            records.extend(new_records)\n",
    "\n",
    "            # collect all records created before the last record's date.\n",
    "            last_record = data[-1]\n",
    "            last_record_date = last_record.get('created_utc')\n",
    "            i += 1\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        if verbose:\n",
    "            print(\"Cancelled early\")\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Exists\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-612593d55673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File Exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     df = pd.read_csv(context['file_subreddit'], \n\u001b[0;32m----> 8\u001b[0;31m                      compression='gzip')\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{ len(df) } Records exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nrows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m     \"\"\"\n\u001b[1;32m    574\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subreddit = 'dankmemes'\n",
    "verbose = True\n",
    "context = get_context(subreddit)\n",
    "\n",
    "if os.path.exists(context['file_subreddit']):\n",
    "    print('File Exists')\n",
    "    df = pd.read_csv(context['file_subreddit'], \n",
    "                     compression='gzip')\n",
    "    \n",
    "    print(f'{ len(df) } Records exist')\n",
    "    min_date = df['created_utc'].min()\n",
    "    max_date = df['created_utc'].max()\n",
    "    seen_ids = set(df['id'].unique())\n",
    "    \n",
    "    newer_records = download_subreddit_posts(subreddit, verbose=verbose, \n",
    "                                             seen_ids=seen_ids,\n",
    "                                             ascending=True, \n",
    "                                             start_date=max_date)\n",
    "    older_records = download_subreddit_posts(subreddit, verbose=verbose, \n",
    "                                             seen_ids=seen_ids,\n",
    "                                             ascending=False, \n",
    "                                             start_date=min_date)\n",
    "    \n",
    "    newer_records.extend(older_records)\n",
    "    \n",
    "    _df = pd.DataFrame(newer_records)\n",
    "    if verbose:\n",
    "        print(f\"collected { len(_df) } records\")\n",
    "\n",
    "    df = df.append(_df, sort=False)\n",
    "    df.drop_duplicates(subset=['id'], inplace=True)\n",
    "    df.sort_values(by=['created_utc'], ascending=False, inplace=True)\n",
    "    df.to_csv(context['file_subreddit'], index=False, compression='gzip')\n",
    "else:\n",
    "    print(\"New Subreddit\")\n",
    "    records = download_subreddit_posts(subreddit, verbose=verbose, size=5000)\n",
    "    if verbose:\n",
    "        print(f\"collected { len(records) } records\")\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(context['file_subreddit'], index=False, compression='gzip')\n",
    "if verbose:\n",
    "    # Summary stats\n",
    "    print('\\n****************')\n",
    "    df['created_at'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "    print(f\"N = { len(df) }\\n\"\n",
    "          f\"Start Date = { df['created_at'].min() }\\n\"\n",
    "          f\"End Date = { df['created_at'].max() }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_media(url, f):\n",
    "    '''\n",
    "    Downloads an image from the net and calcualtes the dhash\n",
    "    '''\n",
    "    if os.path.exists(f):\n",
    "        # is th image exists, don't download it again.\n",
    "        # calculate the size and hash\n",
    "        img_size = os.path.getsize(f)\n",
    "        if img_size != 0:\n",
    "            # read the image and calculate the hash\n",
    "            img = Image.open(f)\n",
    "            dhash = str(imagehash.dhash(img, hash_size=8))\n",
    "            \n",
    "            return dhash, img_size\n",
    "\n",
    "    # Download the image\n",
    "    r = s.get(url, stream=True)\n",
    "    if not r.status_code == 200:\n",
    "        return 'NOHASH', 0\n",
    "    \n",
    "    # download the image locally\n",
    "    with open(f, 'wb') as file:\n",
    "        r.raw.decode_content = True\n",
    "        shutil.copyfileobj(r.raw, file)\n",
    "    \n",
    "    # calculate the hash\n",
    "    img = Image.open(f)\n",
    "    img_size = os.path.getsize(f)\n",
    "    dhash = str(imagehash.dhash(img, hash_size=8))\n",
    "    \n",
    "    return dhash, img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_media_context(image, context):\n",
    "    '''\n",
    "    Establishes where media files will be saved.\n",
    "    '''\n",
    "    image_id = image['id']\n",
    "    pos_images = image.get('resolutions')\n",
    "    if pos_images:\n",
    "        largest_image = pos_images[-1]\n",
    "    else:\n",
    "        # no images...\n",
    "        return None, None\n",
    "    \n",
    "    # where is the image to be downlaoded?\n",
    "    img_url = largest_image.get('url')\n",
    "    img_url = img_url.replace('&amp;', '&')\n",
    "    \n",
    "    # what is the file  extension?\n",
    "    _, ext = os.path.splitext(img_url.split('?')[0])\n",
    "    ext = ext.replace('jpeg', 'jpg')\n",
    "    \n",
    "    # where will the images be downloaded locally?\n",
    "    dir_img = os.path.join(context['media_dir'], \n",
    "                           image_id[:2].lower(), \n",
    "                           image_id[2:4].lower())\n",
    "    \n",
    "    f_img = os.path.join(dir_img, image_id + ext)\n",
    "    os.makedirs(dir_img, exist_ok=True)\n",
    "    \n",
    "    return img_url, f_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(context['file_subreddit'], compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_meta = []\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    preview = row.get('preview')\n",
    "    if isinstance(preview, dict):\n",
    "        images = preview.get('images')\n",
    "        if not images:\n",
    "            continue\n",
    "        for img in images:\n",
    "            r = row.copy()\n",
    "            img_url, f_img = get_media_context(img, context)\n",
    "            if not img_url:\n",
    "                continue\n",
    "            d_hash, img_size = download_media(img_url, f_img)\n",
    "            if img_size != 0:\n",
    "                r['deleted'] = False\n",
    "                r['d_hash'] = d_hash\n",
    "                r['f_img'] = f_img \n",
    "                r['img_size'] = img_size\n",
    "            else:\n",
    "                r['deleted'] = True\n",
    "                r['d_hash'] = d_hash\n",
    "                r['f_img'] = f_img \n",
    "                r['img_size'] = img_size\n",
    "            img_meta.append(r.to_dict())\n",
    "            \n",
    "                \n",
    "df_img_meta = pd.DataFrame(img_meta)\n",
    "df_img_meta.to_json(context['file_subreddit_media'], \n",
    "                    lines=True, orient='records',\n",
    "                    compression='gzip')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-481937901919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_subreddit_media'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'context' is not defined"
     ]
    }
   ],
   "source": [
    "context['file_subreddit_media']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
