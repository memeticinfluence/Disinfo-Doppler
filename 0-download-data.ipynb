{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushshift Reddit Data\n",
    "This Notebook collects subreddit data from PushShift and downloads images from Reddit.\n",
    "\n",
    "## API Endpoints\n",
    "Get a subset of files here:<br>\n",
    "`http://api.pushshift.io/reddit/submission/search/?subreddit=politics&size=500&sort=desc`\n",
    "\n",
    "Latest here:<br>\n",
    "`http://api.pushshift.io/reddit/submission/search/?subreddit=politics&after=2h&size=500`\n",
    "\n",
    "Get everything under the sun here:<br>\n",
    "`files.pushshift.io/reddit/submissions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import requests\n",
    "import itertools\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "import pandas as pd\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[ 502, 503, 504 ])\n",
    "s.mount('http://', HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(subreddit):\n",
    "    '''\n",
    "    Where will data be saved?\n",
    "    '''\n",
    "    data_dir = '/beegfs/work/smapp/reddit_'\n",
    "    sub_dir = os.path.join(data_dir, subreddit)\n",
    "    media_dir =  os.path.join(data_dir, 'media')\n",
    "    file_subreddit = os.path.join(sub_dir, 'posts.json.gz')\n",
    "    file_subreddit_media = os.path.join(sub_dir, 'media.json.gz')\n",
    "    \n",
    "    for _dir in [data_dir, sub_dir, media_dir]:\n",
    "        os.makedirs(_dir, exist_ok=True)\n",
    "        \n",
    "    context = {\n",
    "        'data_dir' : data_dir,\n",
    "        'sub_dir' : sub_dir,\n",
    "        'media_dir' : media_dir,\n",
    "        'file_subreddit' : file_subreddit,\n",
    "        'file_subreddit_media' : file_subreddit_media\n",
    "    }\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query PushShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_api_endpoint(subreddit, size, ascending, last_record_date, verbose):\n",
    "    '''\n",
    "    An easy API endpoint builder for PushShift's Reddit API.\n",
    "    '''\n",
    "    url_base = ('http://api.pushshift.io/reddit/submission/search/'\n",
    "               f'?subreddit={ subreddit }&size={ size }')\n",
    "    if not last_record_date:\n",
    "        url = url_base\n",
    "    else:\n",
    "        if ascending:\n",
    "            url = url_base + f'&after={ last_record_date + 1 }&sort=asc'\n",
    "        else:\n",
    "            url = url_base + f'&before={ last_record_date - 1 }&sort=desc'\n",
    "    if verbose:\n",
    "        print(url)\n",
    "    return url\n",
    "\n",
    "\n",
    "def download_subreddit_posts(subreddit, size=5000, ascending=False, \n",
    "                             start_date=False, seen_ids=set(), \n",
    "                             display_every_x_iterations=20,\n",
    "                             verbose=True):\n",
    "    '''\n",
    "    Downloads subreddit data.\n",
    "    To go back in time from `start_date`, set ascending to False.\n",
    "    To go forward from time `state_date`, set ascneding to True.\n",
    "    Skipps seen_ids\n",
    "    '''\n",
    "    if not isinstance(seen_ids, set):\n",
    "        raise \"seen_ids needs to be a set!\"\n",
    "    i = 0\n",
    "    records = []\n",
    "    last_record_date = start_date\n",
    "    try:\n",
    "        while True:\n",
    "            # Buld the url\n",
    "            url = build_api_endpoint(subreddit, size, ascending, last_record_date, \n",
    "                                     verbose if i % display_every_x_iterations == 0 else False)\n",
    "\n",
    "            # make the HTTP request to the API\n",
    "            r = s.get(url)\n",
    "            resp = r.json()\n",
    "            data = resp.get('data')\n",
    "\n",
    "            # check which records were returned by the API and which are new?\n",
    "            # if there are no new IDs, then we're done!\n",
    "            paginated_ids = {row.get(\"id\") for row in data}\n",
    "            new_ids = paginated_ids - seen_ids\n",
    "            if len(new_ids) == 0:\n",
    "                break\n",
    "\n",
    "            # add new records to existing records. \n",
    "            new_records = [row for row in data if row['id'] in new_ids]\n",
    "            records.extend(new_records)\n",
    "\n",
    "            # collect all records created before the last record's date.\n",
    "            last_record = data[-1]\n",
    "            last_record_date = last_record.get('created_utc')\n",
    "            i += 1\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        if verbose:\n",
    "            print(\"Cancelled early\")\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Exists\n",
      "404231 Records exist\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&after=1554439434&sort=asc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1540950275&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1540431293&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1539923104&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1539455863&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1538949309&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1538590296&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1538107024&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1537578610&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1536890357&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1536205332&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1535505131&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1534805911&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1534029399&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1533382222&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1532672465&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1532019408&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1531420043&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1530753929&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1530150977&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1529607434&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1529004355&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1528409731&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1527696947&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1526994927&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1526142505&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1525351542&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1524657676&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1523897686&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1523127170&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1522343837&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1521655563&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1520911447&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1520185634&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1519498306&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1518916991&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1518280426&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1517672504&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1517341458&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1516803179&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1516308970&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1515725077&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1515090219&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1514342848&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1513666868&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1513087435&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1512510598&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1511998745&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1511320191&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1510774456&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1510224744&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1509759758&sort=desc\n",
      "http://api.pushshift.io/reddit/submission/search/?subreddit=The_Donald&size=5000&before=1509374370&sort=desc\n",
      "Cancelled early\n",
      "collected 1022040 records\n",
      "\n",
      "****************\n",
      "N = 1426271\n",
      "Start Date = 2017-10-29 22:19:32\n",
      "End Date = 2019-04-05 05:22:31\n"
     ]
    }
   ],
   "source": [
    "subreddit = 'The_Donald'\n",
    "verbose = True\n",
    "context = get_context(subreddit)\n",
    "\n",
    "if os.path.exists(context['file_subreddit']):\n",
    "    print('File Exists')\n",
    "    df = pd.read_json(context['file_subreddit'], lines=True, \n",
    "                      orient='records', compression='gzip')\n",
    "    \n",
    "    print(f'{ len(df) } Records exist')\n",
    "    min_date = df['created_utc'].min()\n",
    "    max_date = df['created_utc'].max()\n",
    "    seen_ids = set(df['id'].unique())\n",
    "    \n",
    "    newer_records = download_subreddit_posts(subreddit, verbose=verbose, \n",
    "                                             seen_ids=seen_ids,\n",
    "                                             ascending=True, \n",
    "                                             start_date=max_date)\n",
    "    older_records = download_subreddit_posts(subreddit, verbose=verbose, \n",
    "                                             seen_ids=seen_ids,\n",
    "                                             ascending=False, \n",
    "                                             start_date=min_date)\n",
    "    \n",
    "    newer_records.extend(older_records)\n",
    "    \n",
    "    _df = pd.DataFrame(newer_records)\n",
    "    if verbose:\n",
    "        print(f\"collected { len(_df) } records\")\n",
    "\n",
    "    df = df.append(_df, sort=False)\n",
    "    df.drop_duplicates(subset=['id'], inplace=True)\n",
    "    df.sort_values(by=['created_utc'], ascending=False, inplace=True)\n",
    "    df.to_json(context['file_subreddit'], lines=True, \n",
    "               orient='records', compression='gzip')\n",
    "else:\n",
    "    print(\"New Subreddit\")\n",
    "    records = download_subreddit_posts(subreddit, verbose=verbose, size=5000)\n",
    "    if verbose:\n",
    "        print(f\"collected { len(records) } records\")\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_json(context['file_subreddit'], lines=True, \n",
    "               orient='records', compression='gzip')\n",
    "if verbose:\n",
    "    # Summary stats\n",
    "    print('\\n****************')\n",
    "    df['created_at'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "    print(f\"N = { len(df) }\\n\"\n",
    "          f\"Start Date = { df['created_at'].min() }\\n\"\n",
    "          f\"End Date = { df['created_at'].max() }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_media(url, f):\n",
    "    '''\n",
    "    Downloads an image from the net and calcualtes the dhash\n",
    "    '''\n",
    "    if os.path.exists(f):\n",
    "        # is th image exists, don't download it again.\n",
    "        # calculate the size and hash\n",
    "        img_size = os.path.getsize(f)\n",
    "        if img_size != 0:\n",
    "            # read the image and calculate the hash\n",
    "            img = Image.open(f)\n",
    "            dhash = str(imagehash.dhash(img, hash_size=8))\n",
    "            \n",
    "            return dhash, img_size\n",
    "\n",
    "    # Download the image\n",
    "    r = s.get(url, stream=True)\n",
    "    if not r.status_code == 200:\n",
    "        return 'NOHASH', 0\n",
    "    \n",
    "    # download the image locally\n",
    "    with open(f, 'wb') as file:\n",
    "        r.raw.decode_content = True\n",
    "        shutil.copyfileobj(r.raw, file)\n",
    "    \n",
    "    # calculate the hash\n",
    "    img = Image.open(f)\n",
    "    img_size = os.path.getsize(f)\n",
    "    dhash = str(imagehash.dhash(img, hash_size=8))\n",
    "    \n",
    "    return dhash, img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_media_context(image, context):\n",
    "    '''\n",
    "    Establishes where media files will be saved.\n",
    "    '''\n",
    "    image_id = image['id']\n",
    "    pos_images = image.get('resolutions')\n",
    "    if pos_images:\n",
    "        largest_image = pos_images[-1]\n",
    "    else:\n",
    "        # no images...\n",
    "        return None, None\n",
    "    \n",
    "    # where is the image to be downlaoded?\n",
    "    img_url = largest_image.get('url')\n",
    "    img_url = img_url.replace('&amp;', '&')\n",
    "    \n",
    "    # what is the file  extension?\n",
    "    _, ext = os.path.splitext(img_url.split('?')[0])\n",
    "    ext = ext.replace('jpeg', 'jpg')\n",
    "    \n",
    "    # where will the images be downloaded locally?\n",
    "    dir_img = os.path.join(context['media_dir'], \n",
    "                           image_id[:2].lower(), \n",
    "                           image_id[2:4].lower())\n",
    "    \n",
    "    f_img = os.path.join(dir_img, image_id + ext)\n",
    "    os.makedirs(dir_img, exist_ok=True)\n",
    "    \n",
    "    return img_url, f_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5530it [01:43, 78.51it/s]/home/ly501/anaconda3/lib/python3.6/site-packages/PIL/Image.py:916: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "33190it [20:22, 17.28it/s]"
     ]
    }
   ],
   "source": [
    "# df = pd.read_json(context['file_subreddit'], lines=True, \n",
    "#                   orient='records', compression='gzip')\n",
    "img_meta = []\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    preview = row.get('preview')\n",
    "    if isinstance(preview, dict):\n",
    "        images = preview.get('images')\n",
    "        if not images:\n",
    "            continue\n",
    "        for img in images:\n",
    "            r = row.copy()\n",
    "            img_url, f_img = get_media_context(img, context)\n",
    "            if not img_url:\n",
    "                continue\n",
    "            d_hash, img_size = download_media(img_url, f_img)\n",
    "            if img_size != 0:\n",
    "                r['deleted'] = False\n",
    "                r['d_hash'] = d_hash\n",
    "                r['f_img'] = f_img \n",
    "                r['img_size'] = img_size\n",
    "            else:\n",
    "                r['deleted'] = True\n",
    "                r['d_hash'] = d_hash\n",
    "                r['f_img'] = f_img \n",
    "                r['img_size'] = img_size\n",
    "            img_meta.append(r.to_dict())\n",
    "            \n",
    "                \n",
    "df_img_meta = pd.DataFrame(img_meta)\n",
    "df_img_meta.to_json(context['file_subreddit_media'], \n",
    "                    lines=True, orient='records',\n",
    "                    compression='gzip')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
