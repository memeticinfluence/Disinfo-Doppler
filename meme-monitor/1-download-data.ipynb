{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Pushshift Reddit Data\n",
    "See this on [Github](https://github.com/yinleon/doppler_tutorials/blob/master/1-download-data.ipynb), [NbViewer](https://nbviewer.jupyter.org/github/yinleon/doppler_tutorials/blob/master/1-download-data.ipynb)<br>\n",
    "By Jansen Derr 2021-02-22<br>\n",
    "This Notebook collects subreddit metadata from PushShift's REST API, and downloads images from Reddit using requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "import context_manager\n",
    "import data_sources.pushshift as ps\n",
    "from image_utils import download_media_and_return_dhash, read_image\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trick in data engineering is creating a dictionary (called the _context_) that defines the destination of all output files. Here we illustrate an example of what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_context(subreddit):\n",
    "    '''\n",
    "    Where will data be saved?\n",
    "    '''\n",
    "    sub_dir = os.path.join(config.data_dir, subreddit)\n",
    "    media_dir =  os.path.join(config.data_dir, 'media')\n",
    "    file_subreddit = os.path.join(sub_dir, 'posts.csv.gz')\n",
    "    file_subreddit_media = os.path.join(sub_dir, 'media.csv.gz')\n",
    "    \n",
    "    for _dir in [config.data_dir, sub_dir, media_dir]:\n",
    "        os.makedirs(_dir, exist_ok=True)\n",
    "        \n",
    "    context = {\n",
    "        'data_dir' : config.data_dir,\n",
    "        'sub_dir' : sub_dir,\n",
    "        'media_dir' : media_dir,\n",
    "        'file_subreddit' : file_subreddit,\n",
    "        'file_subreddit_media' : file_subreddit_media\n",
    "    }\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will use functions like `get_subreddit_context` throughout this article. Rather than define these within each notebook, they are all writtein in the `context_manager.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query PushShift\n",
    "Pushshift has an open restful API that returns JSON records. \n",
    "\n",
    "For your convenience, we have written a simple Python wrapper found in in `doppler/data_sources/pushshift.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Note:<br>\n",
    "The Jupyter Notebooks used throughout this article differ a bit from what you might expec. We are making a change by defining code in external Python scripts, rather than in the notebook itself. We do this because it makes the code easier to find and share, and allows notebooks to focus on commentary and explaination of moving parts.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "subreddit = config.subreddit # change this in config.py\n",
    "context = get_subreddit_context(subreddit)\n",
    "min_date = config.start_utc\n",
    "max_date = config.end_utc\n",
    "\n",
    "# check if the subreddit has already been collected\n",
    "if os.path.exists(context['file_subreddit']):\n",
    "    print('File Exists')\n",
    "    df = pd.read_csv(context['file_subreddit'], \n",
    "                     compression='gzip')\n",
    "    print(f'{ len(df) } Records exist')\n",
    "    seen_ids = set(df.id.unique()) # these are records we've already collected\n",
    "    \n",
    "    # look for records that we haven't already collected\n",
    "    records = ps.download_subreddit_posts(subreddit, min_date, max_date,\n",
    "                                                verbose=verbose, \n",
    "                                                seen_ids=seen_ids)\n",
    "    _df = pd.DataFrame(records)\n",
    "    if verbose:\n",
    "        print(f\"collected { len(_df) } records\")\n",
    "    df = df.append(_df, sort=False)\n",
    "    df.drop_duplicates(subset=['id'], inplace=True)\n",
    "    df.sort_values(by=['created_utc'], ascending=False, inplace=True)\n",
    "    df.to_csv(context['file_subreddit'], index=False, compression='gzip')\n",
    "\n",
    "# if we've never colelcted the subreddit, we start a fresh query to download records.\n",
    "else:\n",
    "    print(\"New Subreddit\")\n",
    "    records = ps.download_subreddit_posts(subreddit, min_date, max_date, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(f\"collected { len(records) } records\")\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(context['file_subreddit'], index=False, compression='gzip')\n",
    "\n",
    "if verbose:\n",
    "    # Summary stats\n",
    "    print('\\n****************')\n",
    "    df['created_at'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "    print(f\"N = { len(df) }\\n\"\n",
    "          f\"Start Date = { df['created_at'].min() }\\n\"\n",
    "          f\"End Date = { df['created_at'].max() }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Images\n",
    "After collecting the metadata for posts for our the subreddit of interest, we can download the media shared within the subreddit.\n",
    "In this section, we filter the metadata for media, download all media from the open web, and calculate a fingerprint or _dhash_ of each image. More on dhashing [here](http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html).\n",
    "\n",
    "We created a high-level function called `download_media_and_return_dhash`, which uses the `requests` library to download an image locally, and `imagehash` to calculate the _dhash_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're only interested in records with a media preview...\n",
    "df_media = df[~df.preview.isnull()]\n",
    "len(df_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the file exists, and which media records have been downloaded\n",
    "if os.path.exists(context['file_subreddit_media']):\n",
    "    df_img_meta = pd.read_csv(context['file_subreddit_media'], \n",
    "                              compression='gzip')     \n",
    "    abd = df_img_meta.id\n",
    "    df_media = df_media[~df_media.id.isin(abd)]\n",
    "\n",
    "# download new media files\n",
    "img_meta = []\n",
    "try:\n",
    "    for _, row in tqdm(df_media.iterrows(), position=0, leave=True):\n",
    "        preview = row.get('preview')\n",
    "        if isinstance(preview, dict):\n",
    "            images = preview.get('images')\n",
    "            if not images:\n",
    "                continue\n",
    "            for img in images:\n",
    "                r = row.copy()\n",
    "                img_url, f_img = context_manager.get_media_context(img, context)\n",
    "                if not img_url:\n",
    "                    continue\n",
    "                d_hash, img_size = download_media_and_return_dhash(img_url, f_img)\n",
    "                if img_size != 0:\n",
    "                    r['deleted'] = False\n",
    "                    r['d_hash'] = d_hash\n",
    "                    r['f_img'] = f_img \n",
    "                    r['img_size'] = img_size\n",
    "                else:\n",
    "                    r['deleted'] = True\n",
    "                    r['d_hash'] = d_hash\n",
    "                    r['f_img'] = f_img \n",
    "                    r['img_size'] = img_size\n",
    "                img_meta.append(r.to_dict())\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    if verbose:\n",
    "        print(\"cancelled early!\")\n",
    "    pass\n",
    "\n",
    "\n",
    "# append to existing records, if that exitst and write to a csv\n",
    "if os.path.exists(context['file_subreddit_media']):               \n",
    "    _df_img_meta = pd.DataFrame(img_meta)\n",
    "    df_img_meta = df_img_meta.append(_df_img_meta,sort=False)\n",
    "else:\n",
    "    df_img_meta = pd.DataFrame(img_meta)\n",
    "    \n",
    "df_img_meta.to_csv(context['file_subreddit_media'], \n",
    "                   index=False, compression='gzip')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_img_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the images are downloaded, we can read them from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_img = df_img_meta.f_img.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_image(f_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we calculated the dhash of each image, we can count duplicates. Here are the most re-posted images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_shared_image_dhashed = df_img_meta[df_img_meta.d_hash != 'NOHASH'].d_hash.value_counts().head(20)\n",
    "most_shared_image_dhashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_posted_dhash = most_shared_image_dhashed.index[0]\n",
    "most_posted_image_file = df_img_meta[df_img_meta.d_hash == most_posted_dhash].f_img.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_image(most_posted_image_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
