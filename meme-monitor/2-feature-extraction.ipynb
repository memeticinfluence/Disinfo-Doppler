{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "See this on [Github](https://github.com/yinleon/doppler_tutorials/blob/master/2-feature-extraction.ipynb), [NbViewer](https://nbviewer.jupyter.org/github/yinleon/doppler_tutorials/blob/master/2-feature-extraction.ipynb)<br>\n",
    "By Jansen Derr 2021-02-22<br>\n",
    "\n",
    "In order to power the functions of the Doppler, we need to transform the images we just downloaded into searchable features. We use a neural network that has already been used to a task to create convolutional features called logits. Logits are learned representations of [shapes, colors and patterns](https://distill.pub/2017/feature-visualization/) that neural networks use to differentiate between different types of images through linear regression. We discard the last step of linear regression, so we just have the logits. The distance between the logits of a new image and all existing images determines the relevance of the image search engine. These same relationships are used to cluster and grid images, which we use for mosaic analysis.\n",
    "\n",
    "To do this step-- called `feature extraction`, we use ResNet50 pre-trained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import requests\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from config import cols_conv_feats, skip_hash\n",
    "from image_utils import read_image, read_and_transform_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook needs version >= 0.4.0\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are we using a GPU? If not, the device will be using cpu\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _dir in [config.working_dir, config.media_dir]:\n",
    "    os.makedirs(_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction <a id='features'></a>\n",
    "This section will converting raw data that into ML-friendly data. What that means in this context is downloading images and transforming them into logits formatted as PyTorch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.image_lookup_file, \n",
    "                  compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['d_hash'].isin(skip_hash)] \n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to read images into PyTorch, they need to be [Tensors](https://pytorch.org/docs/stable/tensors.html) with standardized dimensions.<br>For images, the dimensions are (`width`, `height`, `number_of_color_channels`, `batch_size`).\n",
    "\n",
    "When using models that have already been trained, the new inputs need to resemble the input of the original model. For ResNet50, the input dimensions are (224 x 224 x 3). For most models the last dimension (`batch_size`) can be adjusted.\n",
    "\n",
    "torchvision's `transforms` submodule is useful for resizing images, normalizing values and converting the image (which is read into Pillow and NumPy) into a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image needs to be specific dimensions, normalized, and converted to a Tensor to be read into a PyTorch model.\n",
    "scaler = transforms.Resize((224, 224))\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# this is the order of operations that will occur on each image.\n",
    "transformations = transforms.Compose([scaler, \n",
    "                                      to_tensor, \n",
    "                                      normalizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations are called within the `read_and_transform_image` function, which can operate on images on disk or on the web:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that handy function, can convert this local image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = df.f_img.iloc[0]\n",
    "read_image(img_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "into a PyTorch Tensor for ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_transform_image(img_file, transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the above is only operating on one image. To efficiently transform many images use [datasets and dataloaders](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html). Although typically used in training, datasets and dataloaders help parallelize transformations and iterating through the input in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extraction_Dataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and file names\n",
    "    img_col is the column for the image to be read\n",
    "    index_col is a unique value to index the extracted features\n",
    "    \"\"\"\n",
    "    def __init__(self, df, img_col, index_col):\n",
    "        # filter out rows where the file is not on disk.\n",
    "        self.X_train = df.drop_duplicates(subset='d_hash').reset_index(drop=True)\n",
    "        self.files = self.X_train[img_col]\n",
    "        self.idx = self.X_train[index_col]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_idx = self.idx[index]\n",
    "        img_file = self.files[index]\n",
    "        try:\n",
    "            img = read_and_transform_image(self.files[index], transformations)\n",
    "            return img, img_file, img_idx\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abd = []\n",
    "if os.path.exists(config.logits_file):\n",
    "    abd = pd.read_csv(config.logits_file, \n",
    "                      index_col=0).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Feature_Extraction_Dataset(df[~df['d_hash'].isin(abd)], \n",
    "                                     img_col='f_img', \n",
    "                                     index_col='d_hash')\n",
    "data_loader = DataLoader(dataset,\n",
    "                         batch_size=config.batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next load resNet50 pre-trained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resnet_for_feature_extraction():\n",
    "    # Load a pre-trained model\n",
    "    res50_model = models.resnet50(pretrained=True)\n",
    "\n",
    "    # Pop the last Dense layer off. This will give us convolutional features.\n",
    "    res50_conv = nn.Sequential(*list(res50_model.children())[:-1])\n",
    "    res50_conv.to(device)\n",
    "\n",
    "    # Don't run backprop!\n",
    "    for param in res50_conv.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # we won't be training the model. Instead, we just want predictions so we switch to \"eval\" mode. \n",
    "    res50_conv.eval();\n",
    "    \n",
    "    return res50_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50_conv = load_resnet_for_feature_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iterate through the dataset using a data_loader, and convert each batch of images into convolutional feautures. If memory is an issue reduce `batch_size` in the `data_loader`. Data loaders are iterators, for most use cases data loaders are used to return an input (`X`) and a target (`y`) to fit a PyTorch model. We however are not fitting a model, but rather using the data loader in a crucial transformation step in our data pipelines. Thus we return bazaar values such as the path of the image (`img_file`) and the hash (`idx`) instead. X is an array of image Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (X, img_file, idx) in tqdm(data_loader):\n",
    "    X = X.to(device)\n",
    "    logits = res50_conv(X)\n",
    "    #logits.size() # [`batch_size`, 2048, 1, 1])\n",
    "    \n",
    "    logits = logits.squeeze(2) # remove the extra dims\n",
    "    logits = logits.squeeze(2) # remove the extra dims\n",
    "    #logits.size() # [`batch_size`, 2048]\n",
    "    \n",
    "    n_dimensions = logits.size(1)\n",
    "    logits_dict = dict(zip(idx, logits.cpu().data.numpy()))\n",
    "    #{'filename' : np.array([x0, x1, ... x2047])}\n",
    "    \n",
    "    df_conv = pd.DataFrame.from_dict(logits_dict, \n",
    "                                     columns=cols_conv_feats, \n",
    "                                     orient='index')\n",
    "    # add a column for the filename of images...\n",
    "    df_conv['f_img'] = img_file\n",
    "    \n",
    "    # write to file\n",
    "    if os.path.exists(config.logits_file):\n",
    "        df_conv.to_csv(config.logits_file, mode='a', \n",
    "                       header=False, compression='gzip')\n",
    "    else:\n",
    "        df_conv.to_csv(config.logits_file, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Re-run feature extraction on all new images and append them to the `logits_file` csv.\n",
    "\n",
    "Now each image is converted into an array of floats. We maintain the filename in the index to referback to the metadata later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
